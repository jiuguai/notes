# 提取数据
> [IP测试](http://httpbin.org/ip)

> [User-Agent库](http://www.useragentstring.com/pages/useragentstring.php)

> [windows下某些难以安装的模块](https://www.lfd.uci.edu/~gohlke/pythonlibs/)

> [requests](http://docs.python-requests.org/zh_CN/latest/user/advanced.html#proxies)

> [urllib3官网](https://urllib3.readthedocs.io/en/latest/)

> [urllib3](https://blog.csdn.net/LookForDream_/article/details/78624594)

> [aliyun市场](https://www.aliyun.com/ss/6aqM6K-B56CB6K-G5Yir/m/1?spm=5176.11065268.0.0.1df6a1e7B87hub)

> [腾讯云市场](https://market.cloud.tencent.com/search/%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB)

> [selenium](https://selenium-python-zh.readthedocs.io/en/latest/getting-started.html)

> [phantomjs](http://phantomjs.org/api/)


## 代理
+ 带有密码请求头需要添加
    + 'Proxy-Authorization': 'Basic d2luZDEzemVybzo2dmw2cXRxeQ=='
    + Basic d2luZDEzemVybzo2dmw2cXRxeQ== 为账号和密码的base64编码
    + 将代理写成 user:password@IP:port 形式 scrapy 框架会自动处理
```python
PROXIES = [
    '39.98.40.162:16817',
    '47.100.221.224:16817'
]
proxy_authorization = 'Basic ' + base64.b64encode(b'wind13zero:6vl6qtqy').decode()
def process_request(self, request, spider):
    request.headers['Proxy-Authorization'] = self.proxy_authorization
    proxy = random.choice(self.PROXIES)
    request.meta['proxy'] = proxy
    return None

# scrapy 框架中  框架会自动生成 Proxy-Authorization 字段
PROXIES = [
    'wind13zero:6vl6qtqy@39.98.40.162:16817',
    'wind13zero:6vl6qtqy@47.100.221.224:16817'
]
def process_request(self, request, spider):
    proxy = random.choice(self.PROXIES)
    request.meta['proxy'] = proxy
    return None



```

## 格式化输出
```python
# chr(12288) 表示中文空格
# 
print("{k1:{k2}>20}".format('你是',chr(12288)))
print("{k1:{1}>20}".format('你是谁啊',chr(12288)))
print("{k1:{1}>20}".format('你是谁',chr(12288)))

print("{k1:{k2}>20}".format(k1='你是',k2=chr(12288)))
print("{k1:{k2}>20}".format(k1='你是谁啊',k2=chr(12288)))
print("{k1:{k2}>20}".format(k1='你是谁',k2=chr(12288)))
```
## 编码识别
```python
import chardet
x = chardet.detect("我是你把岸边".encode('gb2312'))
print(x)

```

## 信息标记
### YAML
[Online_Edit](https://nodeca.github.io/js-yaml/)

[官网下载](https://pypi.org/project/PyYAML/#history)
```YAML
map:
  # Unordered set of key: value pairs.
  Block style: !!map
    Clark : Evans
    Ingy  : döt Net
    Oren  : Ben-Kiki
  Flow style: !!map { Clark: Evans, Ingy: döt Net, Oren: Ben-Kiki }

# http://yaml.org/type/omap.html ----------------------------------------------#

omap:
  # Explicitly typed ordered map (dictionary).
  Bestiary: !!omap
    - aardvark: African pig-like ant eater. Ugly.
    - anteater: South-American ant eater. Two species.
    - anaconda: South-American constrictor snake. Scaly.
    # Etc.
  # Flow style
  Numbers: !!omap [ one: 1, two: 2, three : 3 ]

# http://yaml.org/type/pairs.html ---------------------------------------------#

pairs:
  # Explicitly typed pairs.
  Block tasks: !!pairs
    - meeting: with team.
    - meeting: with boss.
    - break: lunch.
    - meeting: with client.
  Flow tasks: !!pairs [ meeting: with team, meeting: with boss ]
```


## 正则表达式
    import re
## XPath
### 插件、模块：
    chrome:XPath Helper
    FireFox:try XPath
    pip3 install lxml
### 语法：
#### 谓语部分：
```xpath
    /bookstore/book[1]  选取属于 bookstore 子元素的第一个 book 元素。
    /bookstore/book[last()] 选取属于 bookstore 子元素的最后一个 book 元素。
    /bookstore/book[last()-1]   选取属于 bookstore 子元素的倒数第二个 book 元素。
    /bookstore/book[position()<3]   选取最前面的两个属于 bookstore 元素的子元素的 book 元素。
    //title[@lang]  选取所有拥有名为 lang 的属性的 title 元素。
    //title[@lang='eng']    选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。
    /bookstore/book[price>35.00]    选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。
    /bookstore/book[price>35.00]/title  选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。
    //div[contains(@class,'f1')]
    /bookstore/book[position()<3]/text()
```

## pyquery
>[官方](https://pythonhosted.org/pyquery/)
### 导入方式
```python
from pyquery import PyQuery as pq
```
### 初始化
+ 字符串初始化
```python
doc = pq(html)
```
+ url初始化
```python
doc = pq(url="http://www.baidu.com")
```
+ 文件初始化
```python
doc = pq(filename='demo.html')
```

### 基本CSS选择器
```python

```

### 操作
```python
    # 读取
    # 会补全
    ht = etree.HTML(txt) 
    # 读取外部文件
    #解决哪些不规范的HTML代码
    parser = etree.HTMLParser(encoding='utf-8')
    ht = etree.parse('path',parser=parser)
    # 显示
    etree.tostring(ht,encoding='utf-8').decode()
```

## BeautifuSoup4
[帮助文档](https://beautifulsoup.readthedocs.io/zh_CN/latest/)
### 解析器
   BeautifuSoup(markup,'html.parser') 
   BeautifuSoup(markup,'lxml') 
   BeautifuSoup(markup,'html5lib') 
   xml:
        BeautifuSoup(markup,['lxml',"xml"])
        BeautifuSoup(markup,"xml")
```python
    # 遍历 
    # 下行遍历 (括文本节点)
    .contents 返回列表
    .children    子节点   返回生成器
    .descendants 子孙节点 返回生成器
    # 上行遍历
    .parent
    .parents 
    # 平行遍历 (括文本节点)
    .next_sibling
    .previous_sibling
    .next_siblings
    .previous_siblings

    bs.prettify() #美化输出
    #find or find_all
    tag.string           
    tag.strings          # 返回生成器
    tag.stripped_strings # 不含换行符号
    tag.get_text         # 返回生成器
    tag.attrs

    # select
```
### function
+ .find_all(name,attrs,recursive,string,**kwargs)
    返回一个列表类型
    
    name:对标签名称的检索字符串 str|list|regex
    
    attrs: attr_name|id="link"|regex
    
    secursive:是否搜索子孙几点 default:True,if False 只搜索子节点
    
    string: str|regex

# 获取数据
## urllib 模块
```python
    datau = parse.urlencode(data).encode('utf-8')
    req = request.Request(url,headers=headers,method='POST',data = datau)
    resp = request.urlopen(req)
    t = resp.read().decode()
```

```python
    # 代理
    handler = request.ProxyHandler({'http':'119.28.194.66:8888'})
    opener = request.build_opener(handler)
    req = request.Request(url)
    resp = opener.open(req)
    resp.read().decode()
```

```python
    params = {'name': ['张三'] ,'age':17,'great':'hello world'}

    result = parse.urlencode(params)
    # name=%5B%27%E5%BC%A0%E4%B8%89%27%5D&age=17&great=hello+world

    result = parse.parse_qs(result)
    # {'name': ["['张三']"], 'age': ['17'], 'great': ['hello world']}

    print(parse.quote_from_bytes('age  我'.encode()))
    # age%20%20%E6%88%91

```
## Selenium + chromedriver

>[API](https://selenium-python.readthedocs.io/api.html)

>  Windows 通过 : taskkill /F /im chromedriver.exe 杀死进程
### 安装
    pip3 install selenium

### 驱动下载地址
    1. Chrome
        https://sites.google.com/a/chromium.org/chromedriver/downloads
        放到chrome的安装目录下...\Google\Chrome\Application\ ,然后设置path环境变量

    2. Edge 
        https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/

    3. Firefox
        https://github.com/mozilla/geckodriver/releases
        放到chrome的安装目录下Firefox所在的安装路径，我的是"E:\Mozilla Firefox\"，设置path环境变量
        Path：E:\Mozilla Firefox\;

    4. Safari
        https://webkit.org/blog/6900/webdriver-support-in-safari-10/

### 操作
```python
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    import time

    # 驱动路径
    driver_path = r"E:\Python\driver\chromedriver.exe"
    # 添加驱动路径
    driver = webdriver.Chrome(executable_path=driver_path)

    # 打开页面
    driver.get('https://www.baidu.com')

    # 关闭
    print('沉睡中')
    time.sleep(3)
    # 关闭当前页面
    driver.close()

    # 退出整个浏览
    driver.quit()

    # 页面信息
    print(driver.page_source)

    # 寻找节点
    find_element + s
    search = driver.find_element_by_id('kw')
    search = driver.find_element_by_name('wd')
    search = driver.find_element_by_xpath('wd') # 如果不对元素进行操作 建议 使用lxml :etree
    search = driver.find_element_by_css_selector('#kw')
    search = driver.find_element(By.CSS_SELECTOR,"#kw")
    search = driver.find_element(By.CLASS_NAME,"")
    search = driver.find_element(By.ID,"")
    search = driver.find_element(By.TAG_NAME,"")
    search = driver.find_element(By.NAME,"")


    # 通用操作
    su = driver.find_element_by_id('su')
    search.send_keys('幽梦')
    su.click()
    su.click()
    search.clear()


    # 操作Select
    from selenium.webdriver.support.ui import Select
    select_btn = Select(driver.find_element_by_css_selector(""))
    select_btn.select_by_index(1)
    select_btn.select_by_value("")
    select_btn.select_by_visible_text("中国")
    select_btn.deselect_all() # 取消选择

    # 行为操作
    from selenium.webdriver.common.action_chains import ActionChains
    su = driver.find_element_by_id('su')
    search = driver.find_element_by_id('kw')

    actions = ActionChains(driver)
    actions.move_to_element(search)
    actions.send_keys_to_element(search,'幽梦')
    actions.move_to_element(su)
    actions.click(su)

    # 执行行为链
    actions.perform()

    actions.click_and_hold()
    actions.context_click()
    actions.double_click()
    actions.perform()

    # 获取cookie

    l = driver.get_cookies()
    l = list(l)
    print(l)
    print('='*30)
    print(driver.get_cookie('PSTM'))
    print('='*30)
    driver.add_cookie({})
    driver.delete_cookie('PSTM')
    driver.delete_all_cookies()
    driver.add_cookie({'domain': '.baidu.com', 'expiry': 3686667740.604702, 'httpOnly': False, 'name': 'PSTM', 'path': '/', 'secure': False, 'value': '1539184085'})


    # 页面等待

    # 隐式等待
    driver.implicitly_wait(5)
    s = driver.find_element_by_name("123123")


    # 显示等待:到时间之前还要满足要求就获取
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.support.ui import WebDriverWait
    t = WebDriverWait(driver,5).until(
        EC.presence_of_element_located((By.ID,'kw'))
    )
    print(t)

    # 切换页面
    url = '"https://www.douban.com"'
    driver.execute_script("window.open("+url+")")

    driver.switch_to_window(driver.window_handles[1])
    time.sleep(2)
    driver.close()
    driver.switch_to_window(driver.window_handles[0])
    print(driver.window_handles)
    print(driver.current_url)
    print('执行完毕')

    # 设置代理IP
    options = webdriver.ChromeOptions()
    options.add_argument("--proxy-server=http://115.223.210.133:9000")
    driver = webdriver.Chrome(executable_path=driver_path,options=options)
    driver.get('http://httpbin.org/ip')

    # WebElement

    su = driver.find_element_by_id('su')
    print(su.get_attribute('value'))

    driver.save_screenshot('baidu.png')
    print('执行完毕')
```

## 数据库
### mysql:
```python
    import pymysql

    configure = dict(
        host="localhost",user="root",
        password="root",database="test",
        port=3306,
    )
    conn = pymysql.connect(
        **configure
    )

    cursor = conn.cursor()

    # 查询
    count = cursor.execute("select * from user") # 获取返回记录条数
    print(count)

    result = cursor.fetchone()
    print(result)

    result = cursor.fetchmany(2)
    print(result)

    result = cursor.fetchall()
    print(result)

    sql = """
        insert into user(name,age) values(%s,%s)
    """
    cursor.execute(sql,('dog',3))

    print(conn.insert_id())     # 返回最后一次插入的ID

    # 批量插入
    cursor.executemany(sql,[('test',1000),('fast','1')])

    print(conn.insert_id()) # 批量执行 返回第一次

    # 删除 更新 添加 都需要提交
    conn.commit()

    print(conn.insert_id()) # commit 提交后 结果为0
    print(cursor.lastrowid) # 最后一条主键的ID 受到批量执行的影响

    conn.close()
```

### mongo
```python
    import pymongo

    # 连接
    cli = pymongo.MongoClient("127.0.0.1",port=27017)

    # 创建数据库
    db = cli.zhihu

    # 创建集合
    collection = db.qa

    # 添加
    collection.insert([{"username":"zero",'type':1},
        {"username":"wind",'type':2}

        ])
    collection.insert({"user":"zero"})

    # 查找数据
    ## 返回多条
    cursor = collection.find({'username':"zero"})
    print(list(cursor))
    ## 返回一条
    data1 = collection.find_one({'user':"zero"})
    print(data1)

    # 更新
    collection.update_one({"user":"zero"},{"$set":{"age":18,"type":"test"}})
    collection.update_many({"username":"zero"},{"$set":{"age":18}})

    # 删除
    collection.delete_one({"usernam":"zero"})
    collection.delete_many({"usernam":"zero"})
    cli.close()
```